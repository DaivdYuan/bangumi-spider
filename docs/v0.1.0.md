**!!! 重要: 此版本是最后一个单纯使用网页爬取的版本。此后的版本将同时结合[API](https://github.com/bangumi/api)一并抓取，提高数据质量。!!!**

以下蜘蛛仍然保持不变：
- `bangumi_list`
- `bangumi_anime_list`
- `bangumi_book_list`
- `bangumi_game_list`
- `bangumi_music_list`
- `bangumi_real_list`

下列蜘蛛将同时结合API:
- `bangumi_anime`
- `bangumi_anime_episode` & `bangumi_anime_episode_intro`

注意：新版的数据库将所有的`cn_name`更改为了`name_cn`，以与API统一。

此外，当前版本对于下列数据已经做了预处理：
- 话数
- 放送日期

在之后的版本中此功能将被移除，在后续的工作流中与其他来源的数据库统一实现。

当前的数据库结构 (下面直接展示创建时的SQL命令)：

```sql
CREATE TABLE IF NOT EXISTS `bangumi_id` (
	`sid`		INT UNSIGNED NOT NULL,
	`type`		VARCHAR(10) NOT NULL,
	`name`		VARCHAR(200) NOT NULL,
	`cn_name`	VARCHAR(200) NOT NULL,
	PRIMARY KEY ( `sid` ),
	UNIQUE KEY ( `sid` )
) ENGINE=InnoDB CHARSET=utf8;

CREATE TABLE IF NOT EXISTS `bangumi_anime` (
	`sid`		INT UNSIGNED NOT NULL,
	`name`		VARCHAR(200) NOT NULL,
	`cn_name`	VARCHAR(200) NOT NULL,
	`introHTML`	LONGTEXT,
	`episode`	INT,
	`start`		DATE,
	`attrHTML`	LONGTEXT,
	`tags`		LONGTEXT,
	`type`		VARCHAR(10),
	`thumb`		LONGTEXT,
	PRIMARY KEY ( `sid` ),
	UNIQUE KEY ( `sid` )
) ENGINE=InnoDB CHARSET=utf8;

CREATE TABLE IF NOT EXISTS `bangumi_anime_name` (
	`sid`		INT UNSIGNED NOT NULL,
	`name`		VARCHAR(200) NOT NULL,
	PRIMARY KEY ( `name` ),
	UNIQUE KEY ( `name` )
) ENGINE=InnoDB CHARSET=utf8;

CREATE TABLE IF NOT EXISTS `bangumi_anime_episode` (
	`eid`		INT UNSIGNED NOT NULL,
	`sid`		INT UNSIGNED NOT NULL,
	`name`		VARCHAR(200) NOT NULL,
	`cn_name`	VARCHAR(200) NOT NULL,
	`type`		VARCHAR(10) NOT NULL,
	`order`		INT UNSIGNED NOT NULL,
	`status`	VARCHAR(10) NOT NULL,
	`introHTML`	LONGTEXT,
	PRIMARY KEY ( `eid` ),
	UNIQUE KEY ( `eid` )
) ENGINE=InnoDB CHARSET=utf8;

CREATE TABLE IF NOT EXISTS `request_failed` (
	`id`		INT UNSIGNED NOT NULL,
	`type`		VARCHAR(20) NOT NULL,
	PRIMARY KEY ( `id` ),
	UNIQUE KEY ( `id` )
) ENGINE=InnoDB CHARSET=utf8;
```

此外，此版本还存在一定缺陷，由于即将进行重构，故发布`release`.

下面为原README.

# 0x00 Bangumi Spider [Project Nichijou]

本项目作为项目[Project Nichijou](https://github.com/project-nichijou)中的子项目，是[Bangumi 番组计划](bgm.tv)的爬虫，用于构建番剧数据库。完整内容详见: https://github.com/project-nichijou/intro

## 思路与流程

本repo只包含：
- 数据爬取
- 写入数据库

如果阅读代码可以发现，我们对于大量难以处理的字段使用了直接写入`HTML`的方式，会在后续的流程中 (见[项目架构](https://github.com/project-nichijou/intro)) 进行处理。这样做的原因在于：

1. 提高爬虫速度 (毕竟服务器是小水管 1C2G)
2. 降低在爬取阶段的报错、解析失败、写入失败频率
3. 有利于提高整体工作的稳定性，方便调试
4. 降低数据库复杂度，方便维护

## 关于数据库

本项目目前使用MySQL作为数据库，更多的数据库日后~~可能~~会进行支持，如果有兴趣可以提交PR，持续关注。

此外，当前的默认数据库名称为`bangumi`，本工具会自动新建数据库以及数据表 (若不存在) 。如果和本地数据库名称有冲突，可以在`bangumi/database/database_settings.py`中修改。

## 关于爬虫

本项目实现了如下Spider:

- `bangumi_anime_list`: 爬取动画列表, `/anime/browser/?sort=title&page=<page>`
- `bangumi_book_list`: 爬取书籍列表, `/anime/book/?sort=title&page=<page>`
- `bangumi_game_list`: 爬取游戏列表, `/anime/game/?sort=title&page=<page>`
- `bangumi_music_list`: 爬取音乐列表, `/anime/music/?sort=title&page=<page>`
- `bangumi_real_list`: 爬取三次元列表, `/anime/real/?sort=title&page=<page>`
- `bangumi_anime`: 爬取动画信息, `/subject/<sid>`
- `bangumi_anime_episode`: 爬取动画剧集信息, `/subject/<sid>/ep`
- `bangumi_anime_episode_intro`: 爬取动画剧集介绍, `/ep/<eid>`

因为主项目的性质，故主要精力集中在番剧上面，如果您有其他需要可以自行实现 (欢迎提交PR！)

注意：`bangumi_anime_episode_intro`如果全部爬取会**非常**耗时，所以我们提供了`full`参数，默认为`off`只爬取未播放剧集的信息，`on`时爬取全部信息。

## 环境

- MySQL 5.7.4 +
- Python 3.6 +
- Scrapy
- beautifulsoup4
- Ubuntu (WSL)
- click (optional, 用于构建CLI)

## 配置方法

本项目有两个配置文件：
- `bangumi/bangumi_settings.py`
- `bangumi/database/database_settings.py`

可以发现这两个文件在本`repo`中只有`_template`，需要将这两个`template`配置好并复制、重命名。

关于配置字段的具体含义，文件中都有注释，可以自行查阅。

注意：`bangumi_settings`中的`COOKIES`在某些情况下需要以下字段，~~否则无法爬取特殊内容~~:

```
{
	"chii_auth": <value>
}
```

## 使用方法

经过考量，不准备使用`scrapyd`或者写启动服务器之类的功能，这里只提供了可以用于定时执行的脚本以及`main.py`的CLI工具。我们计划在以后同意实现后端整个工作流的控制管理，不在这里单一实现。目前，可以直接通过以下命令启动爬虫：

```
scrapy crawl <spider_name>
```

`<spider_name>`即为蜘蛛的文件名，位于`bangumi/spider/`目录下。

注意：对于部分蜘蛛，如`bangumi_anime`，有额外的参数。如果需要传参，请使用如下命令：

```
scarpy crawl <spider_name> -a <arg1>=<val1> <arg2>=<val2> ...
```

比如：

```
scrapy crawl bangumi_anime -a fail=off
```

```
scrapy crawl bangumi_anime_episode_intro -a full=on
```

或者也可以使用CLI命令：

```
python3 main.py crawl --help
Usage: main.py crawl [OPTIONS] SPIDER

  start SPIDER crawling using scrapy

  SPIDER: name of the spider to start

Options:
  --fail  whether start in fail mode
  --full  this option is only for episode introduction, whether crawl the full
          list. (extremely time consuming)
  --help  Show this message and exit.
```

## 关于脚本

可以发现，在仓库的根目录我们还提供了下面两个脚本:
- `run_initial.sh`
- `run_cron.sh`

之所以提供脚本其实是因为`scrapy`没有提供定位到特定目录开始任务的命令行参数选项...所以我们就手动实现一下咯。

- `run_initial.sh`: 耗时非常长，适用于初次构建数据库，或者间隔较长时间定时爬取使用。按顺序爬取:
  - `bangumi_anime_list`
  - `bangumi_anime`
  - `bangumi_anime_episode`
  - `bangumi_anime_episode_intro (full)`
- `run_cron.sh`: 相较于上面那个脚本, `bangumi_anime_episode_intro`只进行未播放剧集爬取，正如脚本的名字，适合于定时任务，其他方面并无差别
